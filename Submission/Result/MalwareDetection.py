# For preprocessing
import os
from glob import glob
import statistics
import numpy as np
from pathlib import Path
import json
import sys

# To read model
import pickle

# To write predictions
import csv

# Import both models
static_model_path = os.path.join(os.getcwd(), "models", "static_model")
dynamic_model_path = os.path.join(os.getcwd(), "models", "dynamic_model")
static_model = pickle.load(open(static_model_path, 'rb'))
dynamic_model = pickle.load(open(dynamic_model_path, 'rb'))

# Path to directory
if(sys.argv[1] != None):
    if os.path.exists(sys.argv[1]):
        dir_path = sys.argv[1]
    else:
        sys.exit("Please provide a valid path.")
else:
    sys.exit("usage: python MalwareDetection.py directory_path")

# List of all file paths
filelist = []

# Get list of all files
for root, dirs, files in os.walk(dir_path):
    for file in files:
        if file == "String.txt":
            continue
        # Append file names to list
        filelist.append(os.path.join(root, file))


# PREPROCESSING
# Static files feature extraction

def signs_of_bad_compiler(f_path):
    count = 0
    f = open(f_path, "r", errors="ignore", encoding="utf8")
    lines = f.read()

    count += lines.count("CheckSum:                      0x0")

    count += lines.count("Characteristics:               0x10F")

    split = lines.split("----------PE Sections----------")
    if len(split) != 1:
        split = split[1].split("----------")
        split = split[0].split("\n")
        for line in split:
            split = line.split("0x0   Name:")
            if len(split) != 1:
                name = split[1].strip()
                if name == "":
                    count += 1
    return count


good_props = ["MapViewOfFile", "ResumeThread", "SetWindowsHookExW", "CryptGenRandom",
              "CryptAcquireContextW", "CreateToolhelp32Snapshot", "CertDuplicateCertificateContext"]


def functions_of_benign(f_path):
    count = 0
    f = open(f_path, "r", errors="ignore", encoding="utf8")
    lines = f.read()
    for prop in good_props:
        count += lines.count(prop)
    return count


def num_of_warnings(f_path):
    count = 0
    f = open(f_path, "r", errors="ignore", encoding="utf8")
    lines = f.read()
    count += lines.count("Suspicious flags")
    return count


def calc_entropy(f_path):
    entropy = []
    f = open(f_path, "r", errors="ignore", encoding="utf8")
    lines = f.read()
    entropy_split = lines.split("Entropy: ")
    for each in entropy_split[1:]:
        entropy.append(float(each.split(" ")[0]))
    if not bool(entropy):
        return [0, 0, 0]
    return [min(entropy), max(entropy), statistics.mean(entropy)]


def extract_static_features(f_path):

    features = [0, 0, 0, 0, 0, 0]

    features[0] = signs_of_bad_compiler(f_path)

    features[1] = functions_of_benign(f_path)

    features[2] = num_of_warnings(f_path)

    features[3], features[4], features[5] = calc_entropy(f_path)

    return features

# Dynamic files feature extraction


def max_severity(f_path):
    f = open(f_path, "r", errors="ignore", encoding="utf8")
    f = json.load(f)
    severities = [0]
    for each in f["signatures"]:
        severities.append(each["severity"])
    return max(severities)


def extract_dynamic_features(f_path):
    return [max_severity(f_path)]


# PREDICTION
output_predictions = []

for file in filelist:
    if Path(file).suffix == ".txt":
        # Use static model
        result = static_model.predict([extract_static_features(file)])
        output_predictions.append({
            "Name": file.split("/")[-2],
            "Prediction": "Benign" if result[0] == 0 else "Malware"
        })
    elif Path(file).suffix == ".json":
        # Use dynamic model
        result = dynamic_model.predict([extract_dynamic_features(file)])
        output_predictions.append({
            "Name": file.split("/")[-1].split(".")[0],
            "Prediction": "Benign" if result[0] == 0 else "Malware"
        })

# EXPORTING RESULT


csv_name = "Predictions.csv"
columns = ["Name", "Prediction"]

with open(csv_name, 'w') as csv_file:
    writer = csv.DictWriter(csv_file, fieldnames=columns)
    writer.writeheader()
    for data in output_predictions:
        writer.writerow(data)
print("Saved all predictions in Predictions.csv")
